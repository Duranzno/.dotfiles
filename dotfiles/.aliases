#!/usr/bin/env bash

### enable aliases to be sudoâ€™ed
#
#   http://askubuntu.com/questions/22037/aliases-not-available-when-using-sudo
alias sudo=$'nocorrect sudo	'
alias please="sudo"
alias pls="sudo"
alias plz="sudo"

### ubuntu update & upgrade
#
alias uu="sudo apt-get update && sudo apt-get upgrade -y"
alias uua="sudo apt-get update && sudo apt-get upgrade -y && sudo apt-get autoremove -y"
alias uue="sudo snap refresh && sudo flatpak update -y --noninteractive "

### safety
#
# Avoid stupidity with trash-cli: using https://github.com/sindresorhus/trash-cli
# or use default rm -i if trash is not installed
if type trash &>/dev/null; then
	alias rm="trash"
else
	alias rm="rm -i"
fi

### git
#
alias lg="git log --all --decorate --oneline --graph"


### navigation
#
# go to the /home/$USER (~) directory and clears window of your terminal
alias q="cd ~ && clear"
#
[ -d ~/Development ] && alias pdev="cd ~/Development"
[ -d ~/Downloads ] && alias pd="cd ~/Downloads"

### folders
#
alias ll="ls -AlFh"
alias la="ls -A"
alias l="ls -CF"

### PATH
#
# pretty print $PATH
# shellcheck disable=SC2139,SC2154
alias path="print -l $path"
# shellcheck disable=SC2139,SC2154
alias fpath="print -l $fpath"

### download
#
# web page with all assets
alias getpage="wget --no-clobber --page-requisites --html-extension --convert-links --no-host-directories"
# file with original filename
alias get="curl -O -L"
alias vim="nvim"
alias cl="clear"
# Get tags
function docker-tags () {
  name=$1
  # Initial url
  url=https://registry.hub.docker.com/v2/repositories/$([[ $name =~ "/" ]] && echo $name || echo library/$name)/tags/
  (
    # Keep looping until the variable url is empty
    while [ ! -z $url ]; do
      # Every iteration of the loop prints out a single dot to show progress as it got through all the pages (this is inline dot)
      >&2 echo -n "."
      # Curl the url and pipe the output to python, python will parse the JSON and print the very first line as the next url (it will leave it blank if there is no more pages)
      # then continue to loop over the results extracting only the name; all will be stored in a variable called content
      content=$(curl -s $url | python3 -c 'import sys, json;
data = json.load(sys.stdin); 
print(data.get("next", "") or ""); 
print("\n".join([x["name"] for x in data["results"]]))')
      # Lets get the first line of content which contains the next url for the loop to continue
      url=$(echo "$content" | head -n 1)
      # Print the content without the first line (yes +2 is counter intuitive)
      echo "$content" | tail -n +2
    done;
    # Finally break the line of dots
    >&2 echo
  ) | sort --version-sort | uniq;
}